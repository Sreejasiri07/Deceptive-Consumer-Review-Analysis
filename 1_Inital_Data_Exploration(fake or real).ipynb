{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcnpyDnpy-jx"
      },
      "source": [
        "# INITIAL DATA EXPLORATION"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgF29tNj0kne",
        "outputId": "2735a685-6680-4ce4-a058-3eb444afe2bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv2dvPDuy-j0"
      },
      "source": [
        "This is the first Notebook for this project, and this notebook is going to be utlized to understand the raw Amazon dataset downloaded from Kaggle: https://bit.ly/2Rzvjqf . Here, the dataset was studied, disected and the attributes needed for the next step was analyzed accoridngly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80UbKdSOy-j2"
      },
      "source": [
        "## SUMMARY\n",
        "\n",
        "The initial dataset downloaded from Kaggle had 32 columns, where it had more than enough information. Upon further analysis, the information within the dataset was focused more on the (1) product and (2) the reviews and not so much on the reviewer themselves, and hence it was subsequently concluded that this project will take the lingusitics approach to the fake review detection problem, and not behavioral.\n",
        "\n",
        "The 30 other columns within this dataset, although a bit excessive for the purpose of this project, aided us in understanding the background of the reviews and the products in details, and thus was able to provide us with rich context, which we can refer to when we are analyzing our results down the project. Therefore, before dropping the 30 columns, graphs and analysis were made throughout the Notebook.\n",
        "\n",
        "At the end of the notebook, the columns were dropped, and the only columns kept were review_text and verified_purchase, where they were saved inside a csv file, so we can conduct the EDA and Data Pre-processing on the textual data present."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R9koDkzy-j5"
      },
      "source": [
        "> What is conducted in this notebook?\n",
        "1. Shape of the dataset is studied, and the columns are seen in details.\n",
        "2. Data description and Summary Statistics\n",
        "3. Normalization of data to support EDA analysis\n",
        "4. Checking for NULL and Duplicates\n",
        "5. EDA\n",
        "6. Dropping of columns which are not necessary for model building\n",
        "7. Saving final dataframe to csv."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUzEHGxfy-j8"
      },
      "source": [
        "----------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYMYiITey-kA"
      },
      "source": [
        "## INITIALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNhA2HUzy-kD"
      },
      "outputs": [],
      "source": [
        "#IMPORTING LIBRARIES\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEXa-osqy-kF"
      },
      "outputs": [],
      "source": [
        "#lOADING DATASET\n",
        "df = pd.read_csv(r\"/content/drive/MyDrive/amazon_reviews_2019.csv\",encoding=\"latin1\")\n",
        "#REMOVE MAX\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAsLEoiSy-kH"
      },
      "source": [
        "## SHAPE OF THE DATA AND ITS ATTRIBUTES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRT8H0KPy-kI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "280118d4-e198-473e-fb63-b28b59750775"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Reviews:  2501\n",
            "Total Attributes:  32\n"
          ]
        }
      ],
      "source": [
        "#TOTAL ROWS AND COLUMNS\n",
        "n_reviews, n_attributes = df.shape\n",
        "print('Total Reviews: ',n_reviews)\n",
        "print('Total Attributes: ',n_attributes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoUwe5KQy-kK"
      },
      "source": [
        "> There are in total of 2.5k reviews within this dataset, which makes it to be a valid dataset since the records are more than 1k, thus suitable for analysis.\n",
        "\n",
        "> However, it is to be noted that there are in total of 32 columns within this dataset, which is far more than what is required for this project. These columns need to be taken a deeper look for feature selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U43s0Abxy-kK"
      },
      "outputs": [],
      "source": [
        "#INFORMATION ON THE LOADED CSV FILE. COVERS COLUMN NAMES, TOTAL ROWS AND COLUMS, DATA TYPES AND THE MEMORY USAGE.\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rzDZwoCy-kL"
      },
      "source": [
        "### AT FIRST GLANCE\n",
        "\n",
        "> Most of the attributes in the dataset happens to be of string/object. Since we are going to be reviewing the reviews itself, it is important to note that we have to consider the ones that are needed in regards to the FYP objectives.\n",
        "\n",
        "> To see whether we can go in the direction of behavioral or textual features, the columns should be inspected in depth.\n",
        "\n",
        "> There is a lack of integer values (10 WHEN FLOAT IS COUNTED)  in the dataset, and hence we will not be focusing on numerical features. Need to be seen in further context for more clarification.\n",
        "\n",
        "> there are 21 strings, and hence word cloud and other things can be seen in more depths to study its relationships with one another.\n",
        "\n",
        "> the null values should be taken further look. from first glance, it can be seen that matched_keywords, time_of_publicationm nabyfacturer's response, dimensions 4 - 6, have no value to them. Overall, there seem to be some missing values, which needs to be dealth with\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "vKVw9AmNy-kM"
      },
      "outputs": [],
      "source": [
        "#DISPLAY DATAFRAME\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ytO5Tlay-kN"
      },
      "source": [
        "## DATA DESCRIPTION\n",
        "\n",
        "\n",
        "> We can gain a better grasp of the dataset by digging deeper into the columns. The columns are described in detail below.\n",
        "\n",
        "1. report_date: when the data was extracted initially. From a quick glance, most were collected from the year 2019.\n",
        "2. online_store: The name of the store in which these reviews have been placed.\n",
        "3. upc: The Universal Product Code (UPC) is a barcode symbology that is commonly used to monitor trade products in retailers across the world. Each trade item is issued a unique UPC, which is made up of 12 numeric digits.\n",
        "4. retailer_product_code:  Product identifier from the retailer's side.\n",
        "5. brand: The brand of the product being sold.\n",
        "6. category: the general category of the product e.g foods\n",
        "7. sub_category: the category which describes the category the product is under: category= food, sub-category=savoury\n",
        "8. product_description: description of the product in detail to provide more clarity on the product\n",
        "9. review_date: date when the review is posted.\n",
        "10. review_rating: rating given by the reviewer on the product. Determines the overall score of the review itself, given to the product supposedly purchased. Scale is from 1 - 5, 1 being very poor and 5 being excellent.\n",
        "11. review_title: title given to the review written.\n",
        "12. review_text: the actual review itself, explaining the product supposedly bought by the customer\n",
        "13. is_competitor: whether the prodcut is a competitor or not. Futher analysis should be conducted to understand this column further.\n",
        "14. manufacturer: the manufacturer of the product.\n",
        "15. market: where these products and stores are situated. To understand this further, needs to be taken deeper look.\n",
        "16. matched_keywords: from the dataset, this cannot be derived, since all of the values within this column appears to be NULL.\n",
        "17. time_of_publication: from the dataset, this cannot be derived, since all of the values within this column appears to be NULL.\n",
        "18. url: url of the review itself.\n",
        "19. review_type: indicates the review type. research for more information\n",
        "20. parent_review: indicates whether the review is parent or a child.\n",
        "21. manufacturers_response:\n",
        "22. dimension 1 - 8: miscellenous information on the product itself. There are several missing values within these columns, which will be verified later on.\n",
        "23. verified_purchase: whether the reviews written has been verified or not by Amazon's system.\n",
        "24. helpful_review_count: how many indivudual have found the review to be helpful\n",
        "25. review_hash_id: unique identifier of the review."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9Wojhl1y-kO"
      },
      "source": [
        "## SUMMARY STATISTICS\n",
        "\n",
        "> useful to understand the average values acorss the dataset, and make useful insights regarding its conents. For numerical values, it can aid us in understanding what the max/min values are, and understand the mean values, whilst subsequently understanding if there are any NULL values across the board.\n",
        "\n",
        "> As for the objects, it can aid us in identifying the unique values within the dataset, and how many instances a certain word or character within the columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JG_nNraXy-kO"
      },
      "outputs": [],
      "source": [
        "#NON-OBJECTS\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpZgo9Hvy-kP"
      },
      "source": [
        "1. the average review within this dataset happen to be positive, which can be seen from the high rating of 4.4. The min rating is 1 and the max is 5, 1 being very poor and 5 being excellent.\n",
        "2. most of the reviews within this dataset has not had a helpful review count given to them, which can be seen from the low average of 0.2.\n",
        "3. UPC is a unique identifier, and hence will be ignored in this summary stats\n",
        "4. is_competitor column displays that there happens to be no values indicating that there is anything flagged as a competitor. This can be confirmed by both the min and max values being 0, and hence the average value being 0.\n",
        "5. matched_keywords,\ttime_of_publication,\tmanufacturers_response,\tdimension4,\tdimension5,\tdimension6 all have NULL values entirely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_lB8H7Ny-kP"
      },
      "outputs": [],
      "source": [
        "#OBJECTS\n",
        "df.describe(include=object)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLbTfSPAy-kQ"
      },
      "source": [
        "\n",
        "1. there are in total of 9 stores present in the dataset, most of which are from AMAZON store itself. Most of the stores present happen to be major outlet stores within the UK.\n",
        "2. most of the data happens to come from the category of Personal Care, under the Laundry section of the brand Comfort.\n",
        "3. the reviews where there are mostly positive values can be seen to have \"Good\" in the review_text and \"great value\" written in it, indicating that within the positive reviews have similar wording in them. can be confirmed the more we look into it.\n",
        "4. throught the dataset, there happened to me one manufacturer - Unilever Global\n",
        "5. market happen to only be in UK.\n",
        "6. There are two types of reviews, most of them being organic and parent reviews.\n",
        "7. dimensions 1 - 8 displays extra information on the product itself, as confirmed. most of them happen to be laundry products.\n",
        "8. review_hash_id: this can be ignored since it is a unique identifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmFiueNYy-kQ"
      },
      "outputs": [],
      "source": [
        "df[\"review_type\"].value_counts(normalize=True).to_frame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HHfBR7xy-kQ"
      },
      "source": [
        "Most of the reviews happen to be organic (77%) and the syndicated reviews, which are reffered to reviews of the same product which were shared accorss multiple platforms for higher accessibility, is the minimum (22%)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tck4Oj_Jy-kR"
      },
      "outputs": [],
      "source": [
        "df[\"parent_review\"].value_counts(normalize=True).to_frame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hZlM9NEy-kR"
      },
      "source": [
        "Most of the reviews are said to be parent reviews. Upon further research, it is unclear what this stands for, and hence might be dropped due to is ambiguity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHmuGL0vy-kR"
      },
      "outputs": [],
      "source": [
        "df[\"review_date\"].value_counts().to_frame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W86qRP-Wy-kS"
      },
      "outputs": [],
      "source": [
        "df[\"report_date\"].value_counts().to_frame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVqetDy7y-kS"
      },
      "source": [
        "The years of the reviews  on this dataset consists of 2019 only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKb4urb9y-kS"
      },
      "outputs": [],
      "source": [
        "df.groupby('category')['sub_category'].value_counts().sort_index(ascending=True).to_frame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "351qnZiqy-kT"
      },
      "source": [
        "The sub-categories, grouped by the categories accordingly.\n",
        "\n",
        "Upon closer inspection, there are multiple categories which are overlapping or indicate that they mean the same thing, but are written in differently e.g Hair and Hair care, Deos and Deodorants & Fragrances, or having same sub_categories but different categories e.g Ice Cream under Refreshment and Foods.\n",
        "\n",
        "needs to be handled since this can be useful for further EDA analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsQZLYJGy-kT"
      },
      "outputs": [],
      "source": [
        "#CHANGING THE CATEGORIES AND SUB-CATEGORIES\n",
        "df.loc[df.sub_category == \"Ice Cream\", \"category\"] = \"Refreshment\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Fpuw1x8y-kU"
      },
      "outputs": [],
      "source": [
        "df.loc[df.sub_category == \"HHC\", \"sub_category\"] = \"Household Care\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXSnsIcey-kU"
      },
      "outputs": [],
      "source": [
        "df.loc[df.sub_category == \"Deos\", \"sub_category\"] = \"Deodorants & Fragrances\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swhpwL7Ly-kU"
      },
      "outputs": [],
      "source": [
        "df.loc[df.sub_category == \"Tea\", \"sub_category\"] = \"Tea and Soy & Fruit Beverages\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7adtuZIy-kV"
      },
      "outputs": [],
      "source": [
        "#df.loc[(df['sub_category'] == \"Hair Care\") & (df['category'] == \"Personal Care\")]\n",
        "df.loc[df.sub_category == \"Hair Care\", \"sub_category\"] = \"Hair\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0i7JH5j1y-kX"
      },
      "outputs": [],
      "source": [
        "df.groupby('category')['sub_category'].value_counts().sort_index(ascending=True).to_frame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjduLC0Hy-kY"
      },
      "source": [
        "Everything is now sorted and hence can be used for EDA analysis accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVQ9lEMUy-kY"
      },
      "outputs": [],
      "source": [
        "df[\"online_store\"].value_counts().to_frame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj0i494xy-kZ"
      },
      "source": [
        "Most comes from AMAZON store, and most of them are stores from the UK."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH2ZftRzy-kZ"
      },
      "source": [
        "The majority of the reviews have not been awarded with a helpful review, and if there have been it has been extremely low. Hence, this may skew the results of the reviews itself, and thus would not be helpful in identifying the fake reviews of this dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_woPOrcy-km"
      },
      "source": [
        "## CHECKING FOR NULL AND DUPLICATED VALUES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIzsXgjRy-km"
      },
      "outputs": [],
      "source": [
        "#CHECKING THE NUMBER OF NULL VALUES ACROSS THE DATASET\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo8sCLvGy-kn"
      },
      "source": [
        "Just as mentioned previously, there are NULL values within this dataset which needs to be taken care of.\n",
        "\n",
        "1. review_title has 98 NULL values, which is the minority considering there are over 2k records.\n",
        "2. URL has 800+ missing values, which can be ignored since this is not significant to the nature of our project\n",
        "3. matched_keywords, time_of_publication, manufacturers_response, dimension4, dimension5, dimension6 don't have any values in them, and hence will most definately be dropped.\n",
        "4. dimension 7 has 2 missing values, and from above it can be seen that it contains just an extra info on the product.\n",
        "\n",
        "**There are no NULL values for review_rating, review_text, and verified_purchase, which are the main attributes needed for the analysis.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogkcyDQqy-kn"
      },
      "outputs": [],
      "source": [
        "#DISPLAYS THE NUMBER OF DUPLICATED VALUES\n",
        "dup = df.duplicated().sum()\n",
        "print(\"Number of duplicates in dataset: \", dup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NEWeSWmy-ko"
      },
      "source": [
        "There are no duplicated records within this dataset, therefore this step will be skipped."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOndbj8_y-ko"
      },
      "source": [
        "### VERDICT SO FAR...\n",
        "\n",
        ">There is a lack of behavioral attributes in relation to the reviews itself. There is no information regarding the author, the time of the review written, the collection of the reviews written by the same author, and hence behavioral context cannot be utlized to determine the fake reviews of this project.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6ezOBk4y-ko"
      },
      "source": [
        "### TEXTUAL ATTRIBUTES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khKQcbHJy-kp"
      },
      "outputs": [],
      "source": [
        "#TEXTUAL ATTRIBUTES\n",
        "cols = ['review_date','review_title', 'review_text','review_rating','verified_purchase']\n",
        "textual_attributes = df[cols]\n",
        "pd.DataFrame(textual_attributes).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wdc6ThvQy-kp"
      },
      "source": [
        "\n",
        ">There are textual attributes available within this dataset, and on top of that there are no NULL values within them as displayed above, and hence textual attributes can be used to understand the textual context of these reviews, so by the time we are building the classifier, we have a better undersranding on the review data itself.\n",
        "\n",
        "> essentially the textual attributes can aid us in approaching the problem in a linguistic features fashion, where The linguistic feature considers review text to identifythe reviews as fake vs truth\n",
        "\n",
        ">the attributes above will be used to further our understanding in our EDA process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPB1CcA3y-kp"
      },
      "source": [
        "### OTHER ATTRIBUTES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ilyg4jyGy-kq"
      },
      "outputs": [],
      "source": [
        "#DIMENSIONS 1 - 8\n",
        "cols = ['dimension1', 'dimension2','dimension3','dimension4','dimension5', 'dimension6','dimension7','dimension8']\n",
        "dimensions = df[cols]\n",
        "pd.DataFrame(dimensions).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FfdlNGZy-kq"
      },
      "source": [
        ">The rest of the attributes are considered to be extra information, which has aided us in understanding the background information on the reviews and the products itself. For instance, dimensions 1 - 8 were extra information on the product itself, and although it aided us in understanding the context of the product, it would not be helpful in identifying the fake reviews from the truth reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCWl0-Xqy-kq"
      },
      "source": [
        "## EXPLORATORY DATA ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NB1wRdCky-kr"
      },
      "outputs": [],
      "source": [
        "#SETTING SEABORN STYLE\n",
        "sns.set_style('darkgrid')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jewweihhy-kr"
      },
      "outputs": [],
      "source": [
        "#COUNTPLOT ON TOTAL HELPFUL_REVIEW_COUNTS\n",
        "sns.countplot(x ='helpful_review_count',\n",
        "              data = df).set_title(\"Total Helpful Review Counts in Reviews\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xigciKBZy-ks"
      },
      "source": [
        "In normal basis, helpful_review_count can aid us in understanding which reviews has helped users in purchases a product. However for this dataset, it can be observed that most of the reviews have 0 helpful review count, and others having a miniscule amount of helpdul reviews given. In this case, this will actually skew our understanding in identifying which reviews are fake and which ones are real, and therefore to eliminate bias, this column will not be considered for our model building."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9IZZSk6y-ks"
      },
      "outputs": [],
      "source": [
        "#PIE CHART ON VERFIED PURCHASES - check one\n",
        "colors = ['#79BAEC','#FED8B1']\n",
        "plt.figure(figsize=(4,4))\n",
        "label = df['verified_purchase'].value_counts()\n",
        "plt.pie(label.values,colors = colors, labels=label.index, autopct= '%1.1f%%', startangle=90)\n",
        "plt.title('True and False Reviews Count', fontsize=15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE_SYvCGy-ks"
      },
      "source": [
        "Verified_purchases column is the target variable for this project. From the countplot above, it can be seen that there are near equal parts of true VP and false VP (56% and 44% respectively).\n",
        "\n",
        "Amazon has provided their solution to combat fake reviews by implementing this column, Verified Purchases, where the reviewer has to go through a series of verification steps to ensure that the review that they are placing has indeed been bought from the site. This is Amazon's answer to combating fake reviews, and thus provides security on the truthfulness of the reviews since the review has been placed after purchasing the products.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SK8TuxIey-kt"
      },
      "outputs": [],
      "source": [
        "#COUNTPLOT OF REVIEW RATING GROUPED BY VERIFIED PURCHASE\n",
        "sns.catplot(x ='review_rating',kind=\"count\", hue=\"verified_purchase\",\n",
        "            data=df)\n",
        "plt.xlabel(\"review_rating\")\n",
        "plt.ylabel(\"count of reviews\")\n",
        "plt.title(\"Review_Rating Grouped by Verified_Purchase\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1j9886Oy-kt"
      },
      "source": [
        "According to previous literature, fake reviews refer to reviews that demote/promote a product without any necessary experience with the said service/product.\n",
        "\n",
        "Referring to the graph, it can be observed that False VP has the higest amount of 5 rating compared to its 1 rating, indicating that the fake reviews has been used to promote the product without the purchase. This is usually done to boost the ratings up of the product. It can also be observed that the True Vp has the highest 5 rating compared to the False.\n",
        "\n",
        "When we take a look at the 1 star rating, the False VP happen to be more than the True VP, which can indicate that the reviews were trying to demote the said product, again without previous purchase.\n",
        "\n",
        "Overall, it can be seen that majority of the ratings within this dataset to be positive, and hence should be kept in mind whilst proceeding with the model building."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RhY_PoDy-ku"
      },
      "outputs": [],
      "source": [
        "sns.catplot(x ='category',kind=\"count\", hue = \"verified_purchase\", data=df)\n",
        "plt.xlabel(\"category\")\n",
        "plt.ylabel(\"count of reviews\")\n",
        "plt.title(\"Count of Product Categories\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_gYJryzy-ku"
      },
      "outputs": [],
      "source": [
        "sns.catplot(y ='sub_category',kind=\"count\", hue = \"verified_purchase\", data=df,  height=5, aspect=3)\n",
        "plt.ylabel(\"sub_category\")\n",
        "plt.xlabel(\"count of reviews\")\n",
        "plt.title(\"Count of Product Sub Categories\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zYZegnMy-ku"
      },
      "source": [
        "Looking further into the categories, we can see that the sub-categories with more unverified reviews are under skin care, skin cleansing, personal care and dressings. Most of them are under the Personal care category, with exception to dressings, which falls under the Foods section.\n",
        "\n",
        "fake reviews on beauty products:\n",
        "https://www.dazeddigital.com/beauty/head/article/48461/1/fake-beauty-brand-reviews-sunday-riley-kylie-skin-estee-laundry\n",
        "https://medium.com/swlh/crossing-a-minefield-of-fake-beauty-product-reviews-34af00399668\n",
        "https://edition.cnn.com/2019/10/22/us/sunday-riley-fake-reviews-trnd/index.html\n",
        "https://www.allure.com/story/trust-online-beauty-product-reviews-truth\n",
        "https://www.glamour.com/story/fake-online-beauty-reviews\n",
        "\n",
        "Laundry happens to be the place with the most number of purchases and the highest number of verified purchases, which adds up to the graph above where this sub-cateogory falls under the Home care category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z94iEEOKy-kv"
      },
      "source": [
        "### REVIEW TEXT EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkEJE-mRy-kv"
      },
      "outputs": [],
      "source": [
        "cols = [\"verified_purchase\", \"review_text\"]\n",
        "vprt = df[cols] #making a subset of the dataframe-\n",
        "\n",
        "#FILTERING BASED ON TRUE AND FALSE VP\n",
        "checkTrue = vprt[\"verified_purchase\"] == True\n",
        "filtered_true = vprt[checkTrue]\n",
        "\n",
        "checkFalse = vprt[\"verified_purchase\"] == False\n",
        "filtered_false = vprt[checkFalse]\n",
        "\n",
        "\n",
        "#AVERAGE REVIEW LENGTH BASED ON TRUE AND FALSE VP\n",
        "false_average_length = filtered_false[\"review_text\"].apply(len).mean()\n",
        "true_average_length = filtered_true[\"review_text\"].apply(len).mean()\n",
        "\n",
        "#PLOTTING THE GRAPH\n",
        "x = [true_average_length,false_average_length]\n",
        "y = [\"True\", \"False\"]\n",
        "sns.barplot(x=y, y=x)\n",
        "plt.xlabel(\"average length of reviews\")\n",
        "plt.ylabel(\"verified_purchases\")\n",
        "plt.title(\"Average Length of Reviews based on Verified Purchases\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAik07CCy-kw"
      },
      "source": [
        "One of the key takeaways from this graph is the fact that the average length of the false values happened to exceed the ones which are flagged as verified. This can be backed up by several posts, noteably a site which specilizes in catching fraudulent reviews, where it was mentioned that the average length of these reviews tend to exceed the normal length compared to legitimate reviews.\n",
        "\n",
        "We can see here that the average length of the false VP happen to be more than 250 characters, whilst the true VP happens to be around 50 - 100 characters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CA0Anrbry-kw"
      },
      "source": [
        "## ATTRIBUTES/COLUMNS TO DROP\n",
        "\n",
        ">Now that we are done using the other attributes to further our understanding of the dataset, it is time to consider what is needed for our model building. As mentioned, we are taking the linguistics approach to identifying fraudulent reviews from one another, and hence we are going to keep soly the reviews associated directly with the nature of the reviews: ``review_text, review_date, review_rating, review_title and the verified_purchase columns``.\n",
        "\n",
        ">This is done due to us trying to extract the features from the review_text, and verified_purchases will aid us in identifying which one is the ground false value which can be used to train the classifier. The other review-related columns are going to further aid us in our data pre-processing in the next stage, where we will focus more on the review_text itself and not the other attributes.\n",
        "\n",
        "\n",
        "1. **matched_keywords, time_of_publication, manufacturers_response, dimension4, dimension5, dimension6** contain nothing but NULL values, hence should be droppped.\n",
        "\n",
        "2. **is_competitor and helpful_review_count** have very minimal to zero values within them as displayed in the summary statistics, and hence would not aid us in understanding the dataset or model building, and hence should be dropped.\n",
        "\n",
        "3. **report_date,\tonline_store,brand,\tcategory,\tsub_category, market** columns has aided us in understanding the context of the data and its origin, however will not further aid us in model building.\n",
        "\n",
        "4. **upc, retailer_product_code, review_hash_id, url** are unique identifiers, and hence will not aid us in model building.\n",
        "\n",
        "5. **product_description, parent_review, review_type, manufacturer, dimension1, dimension2,\tdimension3,\tdimension4,\tdimension5** columns are there as extra information on the product itself, and since we are not studying from a product's prespective, these information are renderred useless and will not aid us in our study."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihWqI5e8y-kw"
      },
      "outputs": [],
      "source": [
        "#DROP AFOREMENTIONED COLUMNS\n",
        "df.drop(['matched_keywords', 'time_of_publication'\n",
        "         ,'manufacturers_response', 'dimension4',\n",
        "         'dimension5', 'dimension6',\n",
        "         'is_competitor', 'helpful_review_count',\n",
        "        'report_date','online_store','brand',\n",
        "         'category','sub_category', 'market',\n",
        "        'upc', 'retailer_product_code', 'review_hash_id',\n",
        "         'url','product_description', 'parent_review',\n",
        "         'review_type', 'manufacturer', 'dimension1',\n",
        "         'dimension2','dimension3','dimension4',\n",
        "         'dimension5', 'dimension7',\n",
        "         'dimension8'], axis=1, inplace=True)\n",
        "df.head() #UPDATED DATAFRAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YK4oZ8OEy-kx"
      },
      "outputs": [],
      "source": [
        "#SAVING UPDATED DATAFRAME AS .csv FILE\n",
        "df.to_csv('/content/drive/MyDrive/updated_data.csv')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}